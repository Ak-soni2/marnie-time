# FuelEU Maritime Compliance Platform - Reflection

## What I Learned Using AI Agents
Developing the FuelEU Maritime Compliance Platform with the assistance of AI agents (Lovable, Antigravity, ChatGPT, and Gemini) fundamentally shifted my approach from raw implementation to high-level architectural design and prompt engineering. I learned that AI excels at scaffolding boilerplate structure and generating repetitive code (like standard React components or basic CRUD repository adapters). However, it requires precise, contextual prompting to handle complex, domain-specific business logic accurately—such as the nuanced carbon balancing and pooling mechanisms of the FuelEU regulation. I also realized the importance of iteratively verifying the AI's output; agents can occasionally hallucinate incorrect module imports or misunderstand nuanced testing environments (like TypeScript ESM interop in Jest), requiring human oversight to correct the course.

## Efficiency Gains vs. Manual Coding
The efficiency gains were profound, accelerating the development timeline by roughly 60-70%. Manually scaffolding a full-stack Hexagonal Architecture—with separated domain services, ports, outbound database adapters, and inbound Express controllers—would typically take days of dedicated typing and wiring. Instead, by providing clear architectural constraints, the AI drafted the entire infrastructure in hours. Frontend development saw similar acceleration; tools like Lovable instantly translated structural ideas into styled, functional React components with Tailwind CSS. This allowed me to spend the majority of my time focusing on the core problem: refining the complex FuelEU calculation logic and ensuring accurate data synchronization using TanStack Query, rather than getting bogged down in syntax and boilerplate.

## Improvements I'd Make Next Time
While the AI-assisted workflow was highly productive, there are several areas I would improve in future projects:
1. **Upfront Domain Definition**: I would spend more time explicitly defining the data models, strict types, and boundary constraints in a single "source of truth" document before prompting the AI to generate code. This prevents the agent from making assumptions that have to be refactored later.
2. **Modular Prompting Strategy**: Instead of asking the AI to build entire vertical slices (e.g., "build the whole banking feature"), I would break tasks down into smaller, test-driven prompts (e.g., "write the Banking domain logic and its unit tests first, then write the Postgres adapter").
3. **Advanced Test Generation**: I would rely more heavily on the AI to generate comprehensive integration and edge-case unit tests *before* finalizing the implementation (TDD approach), ensuring the complex maritime algorithms are robustly validated against edge cases from the start.
